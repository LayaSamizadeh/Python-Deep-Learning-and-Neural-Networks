{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exmple data:\n",
    " - experimental drug was tested on individuals ranging from age 13 to 100 in a clinical trial.\n",
    " - The trial had 2100 participants. Half of the participants were under 65 years old, and the other half was 65 years of age or older.\n",
    " - around 95% of patients 65 or older experienced side effects from the drug\n",
    " - around 95% of patients under 65 experienced no side effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    # The ~5% of younger individuals who did exprience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "\n",
    "    # The ~5% of older individuals who did not exprience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "\n",
    "for i in range(1000):\n",
    "    # The ~95% of younger individuals who did not exprience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "\n",
    "    # The ~95% of older individuals who did exprience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "[25, 73, 22, 82, 31, 80, 19, 75, 21, 86, 13, 93, 59, 86, 36, 83, 42, 100, 16, 77, 42, 96, 40, 70, 37, 78, 17, 70, 49, 67, 43, 81, 61, 76, 35, 66, 26, 75, 46, 100, 32, 89, 32, 86, 32, 85, 48, 66, 22, 71, 18, 90, 45, 84, 21, 94, 64, 91, 40, 98, 58, 70, 22, 85, 37, 74, 56, 86, 40, 83, 45, 89, 48, 82, 21, 73, 57, 88, 25, 75, 32, 75, 44, 68, 15, 86, 56, 70, 29, 68, 31, 79, 52, 100, 30, 96, 32, 90, 55, 72, 50, 85, 51, 95, 24, 70, 46, 65, 14, 80, 39, 77, 31, 72, 30, 80, 15, 89, 19, 90, 34, 80, 20, 99, 35, 91, 35, 66, 61, 83, 64, 99, 19, 97, 19, 84, 37, 67, 27, 67, 38, 99, 23, 67, 26, 89, 18, 87, 28, 97, 29, 73, 60, 77, 31, 68, 26, 67, 18, 95, 61, 76, 58, 77, 25, 65, 17, 95, 24, 83, 60, 85, 56, 94, 36, 75, 18, 83, 19, 100, 44, 92, 31, 69, 15, 85, 49, 73, 28, 69, 40, 91, 22, 79, 52, 97, 53, 78, 60, 67, 29, 83, 32, 81, 14, 72, 45, 70, 13, 88, 50, 85, 36, 71, 25, 71, 24, 99, 63, 69, 38, 97, 13, 71, 46, 87, 17, 76, 13, 67, 58, 98, 29, 100, 52, 91, 19, 91, 24, 82, 34, 83, 54, 95, 24, 70, 15, 86, 22, 67, 64, 86, 43, 75, 42, 83, 30, 74, 44, 95, 37, 100, 36, 77, 32, 83, 23, 73, 34, 78, 53, 85, 46, 82, 64, 72, 16, 75, 48, 68, 26, 65, 30, 87, 32, 83, 20, 89, 31, 78, 50, 93, 26, 85, 61, 97, 47, 90, 53, 94, 60, 88, 44, 88, 49, 72, 36, 75, 21, 95, 24, 65, 53, 71, 56, 87, 63, 84, 37, 71, 53, 75, 42, 85, 60, 93, 36, 68, 27, 71, 23, 91, 37, 92, 58, 72, 23, 79, 49, 96, 37, 96, 51, 88, 37, 79, 37, 98, 27, 79, 62, 78, 29, 94, 45, 86, 26, 83, 58, 82, 35, 89, 45, 86, 48, 84, 36, 78, 29, 88, 35, 98, 50, 65, 59, 92, 25, 99, 58, 89, 17, 99, 13, 97, 58, 75, 27, 86, 52, 77, 58, 99, 17, 89, 30, 91, 35, 74, 49, 84, 34, 84, 14, 96, 62, 93, 29, 99, 38, 78, 13, 99, 28, 94, 26, 98, 56, 82, 28, 91, 43, 72, 63, 80, 46, 71, 13, 97, 16, 82, 49, 83, 58, 66, 26, 81, 13, 67, 22, 93, 22, 80, 36, 91, 20, 95, 49, 89, 26, 67, 20, 86, 16, 78, 23, 77, 13, 99, 58, 98, 28, 96, 51, 82, 49, 87, 53, 77, 43, 99, 19, 70, 27, 81, 42, 85, 18, 97, 64, 89, 14, 81, 14, 79, 19, 85, 22, 75, 19, 95, 20, 89, 21, 65, 30, 75, 13, 65, 22, 92, 53, 73, 22, 82, 56, 89, 54, 89, 50, 82, 52, 86, 30, 68, 23, 79, 63, 71, 55, 82, 14, 81, 25, 77, 49, 80, 34, 87, 39, 91, 50, 88, 38, 97, 33, 89, 14, 80, 47, 68, 61, 76, 48, 71, 60, 93, 15, 68, 51, 92, 41, 96, 63, 87, 49, 83, 13, 74, 34, 66, 64, 94, 56, 94, 29, 80, 43, 79, 13, 67, 34, 79, 39, 94, 50, 83, 62, 93, 13, 78, 26, 83, 23, 69, 50, 98, 30, 93, 30, 80, 18, 84, 44, 94, 47, 72, 46, 90, 23, 92, 39, 72, 36, 87, 60, 74, 41, 77, 37, 67, 42, 100, 35, 90, 39, 66, 50, 68, 15, 97, 54, 87, 27, 84, 60, 72, 50, 74, 15, 96, 46, 90, 32, 92, 57, 78, 15, 89, 27, 95, 23, 65, 15, 74, 45, 74, 25, 74, 30, 75, 26, 87, 26, 78, 35, 100, 37, 88, 43, 80, 24, 91, 28, 83, 42, 99, 62, 83, 26, 95, 25, 66, 20, 78, 42, 95, 33, 97, 17, 92, 32, 94, 40, 94, 33, 92, 41, 89, 30, 77, 60, 87, 56, 94, 13, 81, 55, 86, 18, 88, 55, 87, 52, 78, 28, 99, 33, 96, 53, 97, 27, 79, 22, 84, 29, 75, 17, 72, 42, 74, 13, 71, 18, 95, 31, 84, 17, 80, 52, 66, 29, 75, 31, 100, 22, 88, 18, 84, 13, 90, 60, 85, 20, 91, 44, 88, 33, 86, 32, 93, 63, 96, 22, 94, 59, 100, 34, 95, 43, 92, 32, 75, 55, 67, 53, 69, 33, 95, 51, 65, 49, 99, 36, 95, 23, 73, 19, 72, 21, 89, 43, 79, 53, 67, 49, 100, 46, 73, 52, 69, 59, 93, 60, 85, 27, 71, 32, 85, 61, 69, 41, 91, 18, 95, 31, 68, 61, 73, 16, 79, 24, 85, 49, 85, 43, 66, 50, 84, 58, 76, 53, 69, 43, 96, 17, 85, 64, 79, 61, 77, 59, 88, 64, 84, 36, 98, 43, 87, 16, 66, 28, 65, 45, 98, 15, 99, 48, 83, 34, 96, 27, 85, 39, 88, 29, 79, 54, 88, 47, 99, 44, 77, 55, 74, 15, 81, 24, 67, 28, 88, 29, 89, 27, 99, 30, 81, 46, 77, 55, 78, 16, 97, 28, 91, 15, 69, 18, 85, 35, 87, 61, 72, 51, 84, 30, 69, 36, 65, 19, 94, 26, 86, 39, 70, 49, 71, 53, 73, 15, 78, 15, 72, 51, 100, 55, 96, 36, 93, 43, 67, 40, 74, 41, 69, 58, 97, 39, 67, 43, 69, 37, 93, 14, 89, 45, 95, 34, 87, 24, 89, 20, 88, 15, 85, 25, 96, 27, 69, 19, 72, 32, 92, 55, 76, 63, 82, 30, 85, 57, 95, 64, 88, 64, 68, 61, 69, 58, 73, 21, 73, 48, 74, 44, 92, 53, 97, 40, 93, 39, 74, 50, 78, 61, 71, 16, 79, 19, 96, 31, 93, 60, 92, 43, 78, 63, 98, 28, 97, 31, 72, 61, 94, 14, 87, 33, 92, 22, 98, 59, 95, 22, 68, 45, 84, 24, 92, 40, 97, 57, 100, 54, 99, 55, 65, 38, 85, 62, 75, 25, 91, 36, 69, 17, 95, 32, 74, 35, 80, 45, 82, 13, 85, 22, 96, 29, 73, 41, 83, 24, 77, 19, 96, 31, 82, 51, 96, 18, 77, 19, 91, 30, 96, 29, 73, 44, 92, 35, 90, 56, 75, 54, 76, 51, 66, 23, 85, 60, 99, 36, 82, 13, 97, 37, 73, 20, 91, 21, 89, 43, 91, 29, 100, 32, 92, 63, 78, 62, 89, 48, 98, 34, 82, 27, 71, 38, 87, 26, 94, 18, 94, 37, 82, 51, 81, 63, 75, 28, 68, 25, 68, 42, 80, 25, 99, 22, 84, 54, 90, 57, 70, 63, 71, 37, 74, 59, 86, 51, 79, 22, 77, 59, 97, 50, 91, 38, 95, 22, 94, 39, 71, 36, 75, 58, 100, 27, 72, 28, 95, 47, 100, 26, 85, 15, 73, 53, 92, 50, 85, 46, 85, 15, 88, 48, 94, 17, 92, 21, 85, 13, 72, 58, 75, 38, 88, 31, 96, 22, 75, 46, 65, 21, 94, 20, 97, 28, 82, 64, 87, 36, 72, 32, 68, 13, 77, 53, 98, 38, 95, 64, 79, 54, 96, 40, 72, 43, 100, 15, 100, 51, 71, 58, 77, 14, 80, 35, 66, 30, 78, 24, 83, 27, 66, 64, 96, 53, 76, 60, 79, 19, 82, 16, 82, 51, 79, 22, 81, 25, 65, 38, 95, 50, 85, 60, 81, 42, 77, 64, 81, 54, 80, 18, 93, 56, 76, 13, 83, 24, 94, 62, 66, 62, 100, 59, 100, 39, 98, 48, 93, 14, 72, 32, 83, 61, 86, 24, 70, 38, 76, 25, 85, 29, 82, 16, 95, 26, 92, 30, 71, 59, 66, 64, 79, 48, 96, 35, 68, 51, 81, 54, 95, 35, 76, 19, 87, 20, 91, 63, 70, 27, 78, 24, 94, 56, 94, 32, 95, 23, 91, 51, 90, 32, 74, 30, 66, 23, 83, 39, 86, 35, 86, 53, 82, 23, 78, 28, 68, 62, 91, 29, 78, 54, 68, 18, 70, 56, 68, 16, 86, 42, 89, 56, 94, 57, 94, 33, 78, 47, 69, 15, 66, 22, 66, 43, 78, 23, 98, 33, 79, 31, 72, 16, 73, 36, 78, 53, 85, 22, 77, 26, 68, 26, 66, 34, 87, 38, 74, 15, 84, 25, 99, 54, 75, 59, 85, 51, 98, 49, 69, 24, 88, 63, 66, 38, 79, 46, 92, 13, 83, 43, 99, 33, 70, 55, 80, 47, 67, 59, 91, 32, 83, 41, 90, 21, 99, 15, 100, 16, 90, 42, 89, 43, 79, 44, 100, 47, 98, 49, 69, 40, 98, 50, 78, 28, 93, 57, 65, 29, 83, 50, 78, 19, 94, 57, 87, 34, 70, 23, 92, 27, 67, 46, 80, 29, 94, 53, 92, 49, 78, 57, 77, 24, 92, 27, 85, 57, 94, 30, 76, 35, 77, 43, 77, 61, 65, 16, 84, 40, 96, 58, 71, 58, 91, 60, 71, 61, 93, 62, 82, 36, 76, 62, 72, 22, 72, 44, 65, 19, 97, 23, 74, 39, 89, 17, 81, 60, 71, 57, 96, 52, 77, 61, 93, 46, 79, 56, 95, 51, 79, 37, 82, 58, 75, 28, 90, 49, 85, 55, 95, 46, 75, 45, 67, 57, 83, 29, 87, 61, 96, 34, 83, 48, 73, 50, 95, 19, 70, 58, 74, 27, 71, 39, 89, 28, 97, 14, 90, 60, 82, 13, 88, 15, 89, 52, 76, 49, 75, 20, 67, 21, 100, 56, 81, 45, 67, 20, 87, 58, 72, 38, 94, 57, 85, 36, 100, 19, 85, 55, 91, 33, 91, 46, 66, 58, 91, 28, 98, 31, 83, 14, 67, 41, 77, 43, 82, 13, 97, 15, 97, 34, 98, 18, 86, 13, 66, 33, 95, 45, 92, 21, 69, 40, 73, 14, 72, 40, 71, 21, 71, 34, 98, 30, 80, 23, 83, 22, 81, 47, 78, 30, 94, 50, 94, 44, 96, 59, 78, 34, 71, 22, 89, 40, 86, 15, 73, 40, 100, 40, 66, 54, 76, 19, 99, 39, 99, 56, 95, 63, 88, 38, 92, 13, 69, 37, 82, 41, 91, 34, 90, 44, 72, 31, 78, 47, 70, 54, 65, 45, 77, 46, 78, 38, 100, 17, 88, 29, 82, 61, 98, 64, 94, 39, 96, 22, 77, 17, 97, 51, 68, 52, 81, 13, 83, 26, 78, 43, 80, 36, 95, 53, 100, 36, 69, 24, 99, 20, 73, 23, 78, 37, 74, 29, 66, 36, 82, 21, 89, 22, 96, 56, 72, 14, 92, 53, 78, 51, 69, 42, 66, 51, 86, 61, 92, 19, 90, 62, 100, 24, 85, 42, 91, 37, 99, 48, 72, 62, 95, 33, 96, 20, 85, 53, 73, 27, 80, 32, 84, 29, 70, 22, 65, 40, 95, 39, 92, 60, 73, 43, 85, 46, 86, 54, 77, 64, 75, 20, 68, 25, 95, 31, 69, 36, 74, 28, 96, 17, 90, 36, 93, 26, 78, 23, 82, 23, 71, 21, 67, 44, 91, 21, 72, 51, 83, 15, 99, 59, 82, 64, 75, 42, 100, 35, 80, 51, 72, 14, 66, 30, 76, 51, 77, 54, 77, 28, 94, 39, 65, 29, 73, 52, 66, 46, 99, 29, 66, 27, 92, 43, 78, 36, 85, 37, 76, 23, 100, 39, 89, 37, 84, 14, 75, 39, 94, 41, 76, 26, 80, 17, 66, 38, 96, 38, 69, 55, 88, 61, 65, 25, 91, 43, 71, 62, 81, 56, 73, 49, 74, 64, 95, 33, 69, 47, 94, 64, 83, 58, 98, 45, 89, 51, 71, 56, 85, 23, 79, 39, 76, 13, 86, 57, 80, 56, 100, 16, 84, 13, 100, 54, 79, 40, 98, 32, 85, 59, 94, 40, 89, 34, 73, 13, 92, 44, 88, 26, 99, 13, 83, 58, 65, 18, 69, 42, 85, 53, 91, 59, 82, 15, 99, 37, 76, 51, 81, 42, 72, 28, 77, 16, 78, 44, 79, 22, 95, 61, 72, 44, 91, 61, 67, 64, 93, 56, 84, 52, 85, 21, 75, 25, 76, 14, 82, 17, 89, 41, 70, 58, 70, 15, 89, 26, 79, 57, 67, 52, 72, 33, 70, 63, 92, 46, 65, 35, 65, 39, 74, 41, 70, 51, 79, 36, 80, 54, 93, 22, 97, 51, 85, 19, 94, 29, 94, 27, 92, 51, 67, 46, 88, 55, 73, 16, 69, 45, 80, 20, 72, 24, 95, 62, 100, 43, 100, 34, 79, 54, 88, 32, 75, 24, 76, 28, 89, 47, 81, 28, 67, 44, 65, 37, 66, 59, 68, 18, 78, 26, 76, 62, 75, 62, 92, 52, 96]\n"
     ]
    }
   ],
   "source": [
    "print (train_labels) \n",
    "print (train_samples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert both lists into numpy arrays due to what the fit() function expects, and then shuffle the arrays to remove any order that was imposed on the data during the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "train_labels , train_samples = shuffle(train_labels, train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1]\n",
      "[71 57 82 53 91 75 96 80 89 32 94 88 75 71 82 69 80 80 57 19 49 77 62 95\n",
      " 85]\n"
     ]
    }
   ],
   "source": [
    "print (train_labels[:25]) \n",
    "print (train_samples[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667]\n",
      " [0.50574713]\n",
      " [0.79310345]\n",
      " [0.45977011]\n",
      " [0.89655172]\n",
      " [0.71264368]\n",
      " [0.95402299]\n",
      " [0.77011494]\n",
      " [0.87356322]\n",
      " [0.2183908 ]]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train_samples[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple tf.keras Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\Desktop\\Music AI\\Python-Deep-Learning-and-Neural-Networks\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\Desktop\\Music AI\\Python-Deep-Learning-and-Neural-Networks\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                32        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642 (2.51 KB)\n",
      "Trainable params: 642 (2.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\Desktop\\Music AI\\Python-Deep-Learning-and-Neural-Networks\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ASUS\\Desktop\\Music AI\\Python-Deep-Learning-and-Neural-Networks\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "210/210 - 1s - loss: 0.7120 - accuracy: 0.5048 - 748ms/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "210/210 - 0s - loss: 0.6600 - accuracy: 0.7900 - 143ms/epoch - 681us/step\n",
      "Epoch 3/30\n",
      "210/210 - 0s - loss: 0.6154 - accuracy: 0.8186 - 145ms/epoch - 689us/step\n",
      "Epoch 4/30\n",
      "210/210 - 0s - loss: 0.5799 - accuracy: 0.8114 - 144ms/epoch - 685us/step\n",
      "Epoch 5/30\n",
      "210/210 - 0s - loss: 0.5470 - accuracy: 0.8229 - 144ms/epoch - 687us/step\n",
      "Epoch 6/30\n",
      "210/210 - 0s - loss: 0.5166 - accuracy: 0.8452 - 143ms/epoch - 681us/step\n",
      "Epoch 7/30\n",
      "210/210 - 0s - loss: 0.4878 - accuracy: 0.8614 - 146ms/epoch - 693us/step\n",
      "Epoch 8/30\n",
      "210/210 - 0s - loss: 0.4609 - accuracy: 0.8724 - 148ms/epoch - 706us/step\n",
      "Epoch 9/30\n",
      "210/210 - 0s - loss: 0.4362 - accuracy: 0.8786 - 149ms/epoch - 709us/step\n",
      "Epoch 10/30\n",
      "210/210 - 0s - loss: 0.4138 - accuracy: 0.8843 - 144ms/epoch - 684us/step\n",
      "Epoch 11/30\n",
      "210/210 - 0s - loss: 0.3940 - accuracy: 0.8871 - 142ms/epoch - 677us/step\n",
      "Epoch 12/30\n",
      "210/210 - 0s - loss: 0.3764 - accuracy: 0.8929 - 147ms/epoch - 702us/step\n",
      "Epoch 13/30\n",
      "210/210 - 0s - loss: 0.3608 - accuracy: 0.8986 - 147ms/epoch - 702us/step\n",
      "Epoch 14/30\n",
      "210/210 - 0s - loss: 0.3473 - accuracy: 0.9038 - 147ms/epoch - 699us/step\n",
      "Epoch 15/30\n",
      "210/210 - 0s - loss: 0.3355 - accuracy: 0.9095 - 146ms/epoch - 695us/step\n",
      "Epoch 16/30\n",
      "210/210 - 0s - loss: 0.3253 - accuracy: 0.9167 - 145ms/epoch - 688us/step\n",
      "Epoch 17/30\n",
      "210/210 - 0s - loss: 0.3168 - accuracy: 0.9157 - 143ms/epoch - 680us/step\n",
      "Epoch 18/30\n",
      "210/210 - 0s - loss: 0.3093 - accuracy: 0.9176 - 147ms/epoch - 702us/step\n",
      "Epoch 19/30\n",
      "210/210 - 0s - loss: 0.3033 - accuracy: 0.9162 - 147ms/epoch - 698us/step\n",
      "Epoch 20/30\n",
      "210/210 - 0s - loss: 0.2977 - accuracy: 0.9219 - 146ms/epoch - 694us/step\n",
      "Epoch 21/30\n",
      "210/210 - 0s - loss: 0.2929 - accuracy: 0.9262 - 144ms/epoch - 687us/step\n",
      "Epoch 22/30\n",
      "210/210 - 0s - loss: 0.2889 - accuracy: 0.9267 - 144ms/epoch - 688us/step\n",
      "Epoch 23/30\n",
      "210/210 - 0s - loss: 0.2855 - accuracy: 0.9286 - 145ms/epoch - 690us/step\n",
      "Epoch 24/30\n",
      "210/210 - 0s - loss: 0.2824 - accuracy: 0.9286 - 148ms/epoch - 707us/step\n",
      "Epoch 25/30\n",
      "210/210 - 0s - loss: 0.2798 - accuracy: 0.9310 - 149ms/epoch - 709us/step\n",
      "Epoch 26/30\n",
      "210/210 - 0s - loss: 0.2776 - accuracy: 0.9281 - 148ms/epoch - 704us/step\n",
      "Epoch 27/30\n",
      "210/210 - 0s - loss: 0.2757 - accuracy: 0.9314 - 161ms/epoch - 767us/step\n",
      "Epoch 28/30\n",
      "210/210 - 0s - loss: 0.2740 - accuracy: 0.9300 - 145ms/epoch - 692us/step\n",
      "Epoch 29/30\n",
      "210/210 - 0s - loss: 0.2723 - accuracy: 0.9352 - 147ms/epoch - 702us/step\n",
      "Epoch 30/30\n",
      "210/210 - 0s - loss: 0.2710 - accuracy: 0.9319 - 143ms/epoch - 679us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27d04701c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_train_samples, y=train_labels, batch_size=10, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "189/189 - 0s - loss: 0.2655 - accuracy: 0.9360 - val_loss: 0.3083 - val_accuracy: 0.9048 - 283ms/epoch - 1ms/step\n",
      "Epoch 2/30\n",
      "189/189 - 0s - loss: 0.2642 - accuracy: 0.9323 - val_loss: 0.3068 - val_accuracy: 0.9095 - 190ms/epoch - 1ms/step\n",
      "Epoch 3/30\n",
      "189/189 - 0s - loss: 0.2632 - accuracy: 0.9333 - val_loss: 0.3063 - val_accuracy: 0.9095 - 208ms/epoch - 1ms/step\n",
      "Epoch 4/30\n",
      "189/189 - 0s - loss: 0.2622 - accuracy: 0.9376 - val_loss: 0.3055 - val_accuracy: 0.9095 - 196ms/epoch - 1ms/step\n",
      "Epoch 5/30\n",
      "189/189 - 0s - loss: 0.2612 - accuracy: 0.9386 - val_loss: 0.3071 - val_accuracy: 0.9048 - 195ms/epoch - 1ms/step\n",
      "Epoch 6/30\n",
      "189/189 - 0s - loss: 0.2604 - accuracy: 0.9323 - val_loss: 0.3021 - val_accuracy: 0.9095 - 194ms/epoch - 1ms/step\n",
      "Epoch 7/30\n",
      "189/189 - 0s - loss: 0.2596 - accuracy: 0.9392 - val_loss: 0.3036 - val_accuracy: 0.9095 - 180ms/epoch - 953us/step\n",
      "Epoch 8/30\n",
      "189/189 - 0s - loss: 0.2588 - accuracy: 0.9360 - val_loss: 0.3018 - val_accuracy: 0.9095 - 173ms/epoch - 913us/step\n",
      "Epoch 9/30\n",
      "189/189 - 0s - loss: 0.2580 - accuracy: 0.9392 - val_loss: 0.3017 - val_accuracy: 0.9095 - 167ms/epoch - 882us/step\n",
      "Epoch 10/30\n",
      "189/189 - 0s - loss: 0.2571 - accuracy: 0.9392 - val_loss: 0.3016 - val_accuracy: 0.9095 - 171ms/epoch - 904us/step\n",
      "Epoch 11/30\n",
      "189/189 - 0s - loss: 0.2566 - accuracy: 0.9392 - val_loss: 0.3021 - val_accuracy: 0.9048 - 167ms/epoch - 884us/step\n",
      "Epoch 12/30\n",
      "189/189 - 0s - loss: 0.2557 - accuracy: 0.9376 - val_loss: 0.2993 - val_accuracy: 0.9095 - 162ms/epoch - 858us/step\n",
      "Epoch 13/30\n",
      "189/189 - 0s - loss: 0.2551 - accuracy: 0.9370 - val_loss: 0.2993 - val_accuracy: 0.9095 - 169ms/epoch - 892us/step\n",
      "Epoch 14/30\n",
      "189/189 - 0s - loss: 0.2542 - accuracy: 0.9392 - val_loss: 0.2977 - val_accuracy: 0.9095 - 167ms/epoch - 884us/step\n",
      "Epoch 15/30\n",
      "189/189 - 0s - loss: 0.2534 - accuracy: 0.9397 - val_loss: 0.2973 - val_accuracy: 0.9095 - 165ms/epoch - 871us/step\n",
      "Epoch 16/30\n",
      "189/189 - 0s - loss: 0.2529 - accuracy: 0.9392 - val_loss: 0.2961 - val_accuracy: 0.9095 - 174ms/epoch - 919us/step\n",
      "Epoch 17/30\n",
      "189/189 - 0s - loss: 0.2522 - accuracy: 0.9413 - val_loss: 0.2964 - val_accuracy: 0.9095 - 165ms/epoch - 872us/step\n",
      "Epoch 18/30\n",
      "189/189 - 0s - loss: 0.2517 - accuracy: 0.9386 - val_loss: 0.2940 - val_accuracy: 0.9095 - 181ms/epoch - 956us/step\n",
      "Epoch 19/30\n",
      "189/189 - 0s - loss: 0.2510 - accuracy: 0.9402 - val_loss: 0.2936 - val_accuracy: 0.9095 - 181ms/epoch - 959us/step\n",
      "Epoch 20/30\n",
      "189/189 - 0s - loss: 0.2504 - accuracy: 0.9402 - val_loss: 0.2917 - val_accuracy: 0.9095 - 167ms/epoch - 882us/step\n",
      "Epoch 21/30\n",
      "189/189 - 0s - loss: 0.2498 - accuracy: 0.9402 - val_loss: 0.2921 - val_accuracy: 0.9095 - 166ms/epoch - 879us/step\n",
      "Epoch 22/30\n",
      "189/189 - 0s - loss: 0.2493 - accuracy: 0.9397 - val_loss: 0.2918 - val_accuracy: 0.9095 - 166ms/epoch - 877us/step\n",
      "Epoch 23/30\n",
      "189/189 - 0s - loss: 0.2487 - accuracy: 0.9392 - val_loss: 0.2898 - val_accuracy: 0.9190 - 180ms/epoch - 954us/step\n",
      "Epoch 24/30\n",
      "189/189 - 0s - loss: 0.2484 - accuracy: 0.9423 - val_loss: 0.2907 - val_accuracy: 0.9095 - 210ms/epoch - 1ms/step\n",
      "Epoch 25/30\n",
      "189/189 - 0s - loss: 0.2477 - accuracy: 0.9413 - val_loss: 0.2905 - val_accuracy: 0.9095 - 187ms/epoch - 991us/step\n",
      "Epoch 26/30\n",
      "189/189 - 0s - loss: 0.2472 - accuracy: 0.9392 - val_loss: 0.2889 - val_accuracy: 0.9095 - 166ms/epoch - 879us/step\n",
      "Epoch 27/30\n",
      "189/189 - 0s - loss: 0.2469 - accuracy: 0.9434 - val_loss: 0.2883 - val_accuracy: 0.9095 - 167ms/epoch - 884us/step\n",
      "Epoch 28/30\n",
      "189/189 - 0s - loss: 0.2465 - accuracy: 0.9402 - val_loss: 0.2887 - val_accuracy: 0.9095 - 165ms/epoch - 874us/step\n",
      "Epoch 29/30\n",
      "189/189 - 0s - loss: 0.2461 - accuracy: 0.9397 - val_loss: 0.2878 - val_accuracy: 0.9095 - 165ms/epoch - 873us/step\n",
      "Epoch 30/30\n",
      "189/189 - 0s - loss: 0.2456 - accuracy: 0.9402 - val_loss: 0.2865 - val_accuracy: 0.9190 - 175ms/epoch - 923us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27d047b1390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "      x=scaled_train_samples\n",
    "    , y=train_labels\n",
    "    , validation_split=0.1\n",
    "    , batch_size=10\n",
    "    , epochs=30\n",
    "    , verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "test_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # The ~5% of younger individuals who did exprience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "\n",
    "    # The ~5% of older individuals who did not exprience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "\n",
    "for i in range(200):\n",
    "    # The ~95% of younger individuals who did not exprience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "\n",
    "    # The ~95% of older individuals who did exprience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "test_labels , test_samples = shuffle( test_labels, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=scaled_test_samples, batch_size=10, verbose=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02512425 0.97487575]\n",
      "[0.97195405 0.02804594]\n",
      "[0.30881542 0.6911845 ]\n",
      "[0.02166159 0.9783384 ]\n",
      "[0.88027734 0.11972269]\n",
      "[0.7376036  0.26239643]\n",
      "[0.045169   0.95483106]\n",
      "[0.10513654 0.8948635 ]\n",
      "[0.16223802 0.837762  ]\n",
      "[0.9742278  0.02577218]\n",
      "[0.045169   0.95483106]\n",
      "[0.10513654 0.8948635 ]\n",
      "[0.84584206 0.15415795]\n",
      "[0.34559438 0.6544056 ]\n",
      "[0.0521889 0.9478111]\n",
      "[0.06467446 0.9353255 ]\n",
      "[0.97416    0.02583996]\n",
      "[0.03134909 0.96865094]\n",
      "[0.97214556 0.02785443]\n",
      "[0.9512524  0.04874762]\n",
      "[0.02912395 0.97087604]\n",
      "[0.01866703 0.98133296]\n",
      "[0.02705233 0.97294766]\n",
      "[0.2423084  0.75769156]\n",
      "[0.01866703 0.98133296]\n",
      "[0.16223802 0.837762  ]\n",
      "[0.5901887  0.40981126]\n",
      "[0.9741261 0.0258739]\n",
      "[0.82275915 0.17724088]\n",
      "[0.21294455 0.7870554 ]\n",
      "[0.08565727 0.9143427 ]\n",
      "[0.9727126  0.02728745]\n",
      "[0.02705233 0.97294766]\n",
      "[0.03630286 0.9636972 ]\n",
      "[0.9727126  0.02728745]\n",
      "[0.05607449 0.94392544]\n",
      "[0.6679988 0.3320012]\n",
      "[0.84584206 0.15415795]\n",
      "[0.06942156 0.9305784 ]\n",
      "[0.30881542 0.6911845 ]\n",
      "[0.02912395 0.97087604]\n",
      "[0.97176117 0.02823875]\n",
      "[0.04200544 0.95799464]\n",
      "[0.01732595 0.98267406]\n",
      "[0.02912395 0.97087604]\n",
      "[0.82275915 0.17724088]\n",
      "[0.972899   0.02710097]\n",
      "[0.08565727 0.9143427 ]\n",
      "[0.03134909 0.96865094]\n",
      "[0.9742278  0.02577218]\n",
      "[0.5075853  0.49241477]\n",
      "[0.09243771 0.90756226]\n",
      "[0.09243771 0.90756226]\n",
      "[0.02166159 0.9783384 ]\n",
      "[0.9711892  0.02881083]\n",
      "[0.9152649  0.08473512]\n",
      "[0.9741261 0.0258739]\n",
      "[0.97214556 0.02785443]\n",
      "[0.9741261 0.0258739]\n",
      "[0.10513654 0.8948635 ]\n",
      "[0.21294455 0.7870554 ]\n",
      "[0.34559438 0.6544056 ]\n",
      "[0.05607449 0.94392544]\n",
      "[0.9742955  0.02570459]\n",
      "[0.04855872 0.9514412 ]\n",
      "[0.97176117 0.02823875]\n",
      "[0.97176117 0.02823875]\n",
      "[0.01732595 0.98267406]\n",
      "[0.27430987 0.72569007]\n",
      "[0.9725248  0.02747519]\n",
      "[0.03373832 0.9662616 ]\n",
      "[0.02512425 0.97487575]\n",
      "[0.9668236  0.03317642]\n",
      "[0.7376036  0.26239643]\n",
      "[0.0798953 0.9201047]\n",
      "[0.03134909 0.96865094]\n",
      "[0.0798953 0.9201047]\n",
      "[0.79704916 0.20295085]\n",
      "[0.03905441 0.96094555]\n",
      "[0.9152649  0.08473512]\n",
      "[0.84584206 0.15415795]\n",
      "[0.9723359  0.02766417]\n",
      "[0.84584206 0.15415795]\n",
      "[0.9725248  0.02747519]\n",
      "[0.12173822 0.87826174]\n",
      "[0.03905441 0.96094555]\n",
      "[0.04855872 0.9514412 ]\n",
      "[0.93262166 0.0673784 ]\n",
      "[0.9727126  0.02728745]\n",
      "[0.972899   0.02710097]\n",
      "[0.38431785 0.6156822 ]\n",
      "[0.0233303  0.97666967]\n",
      "[0.06942156 0.9305784 ]\n",
      "[0.09243771 0.90756226]\n",
      "[0.9723359  0.02766417]\n",
      "[0.0233303  0.97666967]\n",
      "[0.0233303  0.97666967]\n",
      "[0.01866703 0.98133296]\n",
      "[0.97426164 0.02573837]\n",
      "[0.03905441 0.96094555]\n",
      "[0.42456555 0.5754344 ]\n",
      "[0.97176117 0.02823875]\n",
      "[0.9512524  0.04874762]\n",
      "[0.03373832 0.9662616 ]\n",
      "[0.06023101 0.93976897]\n",
      "[0.02912395 0.97087604]\n",
      "[0.9729636  0.02703637]\n",
      "[0.06023101 0.93976897]\n",
      "[0.9727126  0.02728745]\n",
      "[0.04855872 0.9514412 ]\n",
      "[0.9736327  0.02636734]\n",
      "[0.86609346 0.13390653]\n",
      "[0.9741702  0.02582983]\n",
      "[0.97434694 0.02565305]\n",
      "[0.0521889 0.9478111]\n",
      "[0.97345114 0.02654892]\n",
      "[0.02705233 0.97294766]\n",
      "[0.9732683  0.02673171]\n",
      "[0.045169   0.95483106]\n",
      "[0.03630286 0.9636972 ]\n",
      "[0.9741702  0.02582983]\n",
      "[0.89314413 0.10685585]\n",
      "[0.6299371 0.3700629]\n",
      "[0.0521889 0.9478111]\n",
      "[0.96414065 0.03585937]\n",
      "[0.0521889 0.9478111]\n",
      "[0.02912395 0.97087604]\n",
      "[0.04200544 0.95799464]\n",
      "[0.96414065 0.03585937]\n",
      "[0.9743628  0.02563716]\n",
      "[0.07448935 0.92551064]\n",
      "[0.0521889 0.9478111]\n",
      "[0.97214556 0.02785443]\n",
      "[0.7376036  0.26239643]\n",
      "[0.97195405 0.02804594]\n",
      "[0.0521889 0.9478111]\n",
      "[0.549226 0.450774]\n",
      "[0.05607449 0.94392544]\n",
      "[0.02166159 0.9783384 ]\n",
      "[0.9729636  0.02703637]\n",
      "[0.9743628  0.02563716]\n",
      "[0.1407748 0.8592252]\n",
      "[0.9741261 0.0258739]\n",
      "[0.03905441 0.96094555]\n",
      "[0.5075853  0.49241477]\n",
      "[0.12173822 0.87826174]\n",
      "[0.9152649  0.08473512]\n",
      "[0.76865846 0.23134154]\n",
      "[0.972899   0.02710097]\n",
      "[0.1407748 0.8592252]\n",
      "[0.06023101 0.93976897]\n",
      "[0.9741702  0.02582983]\n",
      "[0.972899   0.02710097]\n",
      "[0.974194   0.02580605]\n",
      "[0.9743628  0.02563716]\n",
      "[0.7039855  0.29601452]\n",
      "[0.9607592  0.03924081]\n",
      "[0.9736327  0.02636734]\n",
      "[0.7376036  0.26239643]\n",
      "[0.9711892  0.02881083]\n",
      "[0.09243771 0.90756226]\n",
      "[0.9668236  0.03317642]\n",
      "[0.97176117 0.02823875]\n",
      "[0.9727126  0.02728745]\n",
      "[0.6299371 0.3700629]\n",
      "[0.5075853  0.49241477]\n",
      "[0.12173822 0.87826174]\n",
      "[0.9742278  0.02577218]\n",
      "[0.04855872 0.9514412 ]\n",
      "[0.9711892  0.02881083]\n",
      "[0.9741702  0.02582983]\n",
      "[0.6299371 0.3700629]\n",
      "[0.9725248  0.02747519]\n",
      "[0.0798953 0.9201047]\n",
      "[0.38431785 0.6156822 ]\n",
      "[0.01732595 0.98267406]\n",
      "[0.95629114 0.04370888]\n",
      "[0.06942156 0.9305784 ]\n",
      "[0.06023101 0.93976897]\n",
      "[0.08565727 0.9143427 ]\n",
      "[0.02010979 0.9798902 ]\n",
      "[0.02912395 0.97087604]\n",
      "[0.9246055  0.07539455]\n",
      "[0.97399217 0.02600781]\n",
      "[0.9743292  0.02567086]\n",
      "[0.9723359  0.02766417]\n",
      "[0.2423084  0.75769156]\n",
      "[0.01732595 0.98267406]\n",
      "[0.97381306 0.02618697]\n",
      "[0.9152649  0.08473512]\n",
      "[0.16223802 0.837762  ]\n",
      "[0.5901887  0.40981126]\n",
      "[0.16223802 0.837762  ]\n",
      "[0.38431785 0.6156822 ]\n",
      "[0.0521889 0.9478111]\n",
      "[0.02010979 0.9798902 ]\n",
      "[0.27430987 0.72569007]\n",
      "[0.09243771 0.90756226]\n",
      "[0.9741702  0.02582983]\n",
      "[0.9742955  0.02570459]\n",
      "[0.08565727 0.9143427 ]\n",
      "[0.9736327  0.02636734]\n",
      "[0.16223802 0.837762  ]\n",
      "[0.82275915 0.17724088]\n",
      "[0.03373832 0.9662616 ]\n",
      "[0.9743628  0.02563716]\n",
      "[0.08565727 0.9143427 ]\n",
      "[0.0233303  0.97666967]\n",
      "[0.03630286 0.9636972 ]\n",
      "[0.04200544 0.95799464]\n",
      "[0.02166159 0.9783384 ]\n",
      "[0.9732683  0.02673171]\n",
      "[0.9736327  0.02636734]\n",
      "[0.549226 0.450774]\n",
      "[0.6299371 0.3700629]\n",
      "[0.18626446 0.8137356 ]\n",
      "[0.86609346 0.13390653]\n",
      "[0.94566584 0.05433422]\n",
      "[0.97434694 0.02565305]\n",
      "[0.02166159 0.9783384 ]\n",
      "[0.9730843  0.02691572]\n",
      "[0.02705233 0.97294766]\n",
      "[0.6679988 0.3320012]\n",
      "[0.42456555 0.5754344 ]\n",
      "[0.03905441 0.96094555]\n",
      "[0.06023101 0.93976897]\n",
      "[0.03905441 0.96094555]\n",
      "[0.9741702  0.02582983]\n",
      "[0.94566584 0.05433422]\n",
      "[0.02166159 0.9783384 ]\n",
      "[0.9730843  0.02691572]\n",
      "[0.38431785 0.6156822 ]\n",
      "[0.5901887  0.40981126]\n",
      "[0.97381306 0.02618697]\n",
      "[0.08565727 0.9143427 ]\n",
      "[0.5075853  0.49241477]\n",
      "[0.04200544 0.95799464]\n",
      "[0.03630286 0.9636972 ]\n",
      "[0.10513654 0.8948635 ]\n",
      "[0.18626446 0.8137356 ]\n",
      "[0.76865846 0.23134154]\n",
      "[0.2423084  0.75769156]\n",
      "[0.9743292  0.02567086]\n",
      "[0.97345114 0.02654892]\n",
      "[0.9711892  0.02881083]\n",
      "[0.6299371 0.3700629]\n",
      "[0.03630286 0.9636972 ]\n",
      "[0.46583873 0.53416127]\n",
      "[0.97176117 0.02823875]\n",
      "[0.05607449 0.94392544]\n",
      "[0.89314413 0.10685585]\n",
      "[0.02705233 0.97294766]\n",
      "[0.82275915 0.17724088]\n",
      "[0.6679988 0.3320012]\n",
      "[0.9729636  0.02703637]\n",
      "[0.97434694 0.02565305]\n",
      "[0.5901887  0.40981126]\n",
      "[0.06942156 0.9305784 ]\n",
      "[0.02166159 0.9783384 ]\n",
      "[0.03134909 0.96865094]\n",
      "[0.01732595 0.98267406]\n",
      "[0.09243771 0.90756226]\n",
      "[0.9668236  0.03317642]\n",
      "[0.82275915 0.17724088]\n",
      "[0.02512425 0.97487575]\n",
      "[0.97381306 0.02618697]\n",
      "[0.9668236  0.03317642]\n",
      "[0.01866703 0.98133296]\n",
      "[0.04855872 0.9514412 ]\n",
      "[0.9741702  0.02582983]\n",
      "[0.02705233 0.97294766]\n",
      "[0.9729636  0.02703637]\n",
      "[0.9725248  0.02747519]\n",
      "[0.9729636  0.02703637]\n",
      "[0.9741261 0.0258739]\n",
      "[0.06942156 0.9305784 ]\n",
      "[0.9152649  0.08473512]\n",
      "[0.9152649  0.08473512]\n",
      "[0.82275915 0.17724088]\n",
      "[0.90477777 0.09522228]\n",
      "[0.06467446 0.9353255 ]\n",
      "[0.97345114 0.02654892]\n",
      "[0.04200544 0.95799464]\n",
      "[0.03630286 0.9636972 ]\n",
      "[0.974194   0.02580605]\n",
      "[0.6299371 0.3700629]\n",
      "[0.9607592  0.03924081]\n",
      "[0.21294455 0.7870554 ]\n",
      "[0.38431785 0.6156822 ]\n",
      "[0.96414065 0.03585937]\n",
      "[0.01732595 0.98267406]\n",
      "[0.05607449 0.94392544]\n",
      "[0.12173822 0.87826174]\n",
      "[0.34559438 0.6544056 ]\n",
      "[0.9742955  0.02570459]\n",
      "[0.21294455 0.7870554 ]\n",
      "[0.97416    0.02583996]\n",
      "[0.07448935 0.92551064]\n",
      "[0.9711892  0.02881083]\n",
      "[0.045169   0.95483106]\n",
      "[0.95629114 0.04370888]\n",
      "[0.045169   0.95483106]\n",
      "[0.7376036  0.26239643]\n",
      "[0.97416    0.02583996]\n",
      "[0.96414065 0.03585937]\n",
      "[0.9723359  0.02766417]\n",
      "[0.18626446 0.8137356 ]\n",
      "[0.05607449 0.94392544]\n",
      "[0.9512524  0.04874762]\n",
      "[0.97381306 0.02618697]\n",
      "[0.97416    0.02583996]\n",
      "[0.93262166 0.0673784 ]\n",
      "[0.04200544 0.95799464]\n",
      "[0.04200544 0.95799464]\n",
      "[0.0233303  0.97666967]\n",
      "[0.02705233 0.97294766]\n",
      "[0.02512425 0.97487575]\n",
      "[0.0798953 0.9201047]\n",
      "[0.01866703 0.98133296]\n",
      "[0.21294455 0.7870554 ]\n",
      "[0.84584206 0.15415795]\n",
      "[0.21294455 0.7870554 ]\n",
      "[0.97399217 0.02600781]\n",
      "[0.0233303  0.97666967]\n",
      "[0.045169   0.95483106]\n",
      "[0.10513654 0.8948635 ]\n",
      "[0.9246055  0.07539455]\n",
      "[0.03630286 0.9636972 ]\n",
      "[0.7039855  0.29601452]\n",
      "[0.01732595 0.98267406]\n",
      "[0.9725248  0.02747519]\n",
      "[0.34559438 0.6544056 ]\n",
      "[0.974194   0.02580605]\n",
      "[0.97195405 0.02804594]\n",
      "[0.97195405 0.02804594]\n",
      "[0.03905441 0.96094555]\n",
      "[0.9732683  0.02673171]\n",
      "[0.42456555 0.5754344 ]\n",
      "[0.79704916 0.20295085]\n",
      "[0.9711892  0.02881083]\n",
      "[0.90477777 0.09522228]\n",
      "[0.12173822 0.87826174]\n",
      "[0.30881542 0.6911845 ]\n",
      "[0.01866703 0.98133296]\n",
      "[0.6679988 0.3320012]\n",
      "[0.9742278  0.02577218]\n",
      "[0.90477777 0.09522228]\n",
      "[0.9668236  0.03317642]\n",
      "[0.97399217 0.02600781]\n",
      "[0.9607592  0.03924081]\n",
      "[0.9729636  0.02703637]\n",
      "[0.9668236  0.03317642]\n",
      "[0.97176117 0.02823875]\n",
      "[0.9736327  0.02636734]\n",
      "[0.97416    0.02583996]\n",
      "[0.08565727 0.9143427 ]\n",
      "[0.89314413 0.10685585]\n",
      "[0.97399217 0.02600781]\n",
      "[0.95629114 0.04370888]\n",
      "[0.02912395 0.97087604]\n",
      "[0.06023101 0.93976897]\n",
      "[0.07448935 0.92551064]\n",
      "[0.9732683  0.02673171]\n",
      "[0.0798953 0.9201047]\n",
      "[0.01732595 0.98267406]\n",
      "[0.90477777 0.09522228]\n",
      "[0.9727126  0.02728745]\n",
      "[0.9742955  0.02570459]\n",
      "[0.10513654 0.8948635 ]\n",
      "[0.97195405 0.02804594]\n",
      "[0.90477777 0.09522228]\n",
      "[0.86609346 0.13390653]\n",
      "[0.12173822 0.87826174]\n",
      "[0.9743292  0.02567086]\n",
      "[0.09243771 0.90756226]\n",
      "[0.06467446 0.9353255 ]\n",
      "[0.0521889 0.9478111]\n",
      "[0.6299371 0.3700629]\n",
      "[0.9725248  0.02747519]\n",
      "[0.02705233 0.97294766]\n",
      "[0.9152649  0.08473512]\n",
      "[0.9741702  0.02582983]\n",
      "[0.02705233 0.97294766]\n",
      "[0.02705233 0.97294766]\n",
      "[0.9607592  0.03924081]\n",
      "[0.02010979 0.9798902 ]\n",
      "[0.93262166 0.0673784 ]\n",
      "[0.97195405 0.02804594]\n",
      "[0.03905441 0.96094555]\n",
      "[0.0233303  0.97666967]\n",
      "[0.01866703 0.98133296]\n",
      "[0.9607592  0.03924081]\n",
      "[0.7376036  0.26239643]\n",
      "[0.0233303  0.97666967]\n",
      "[0.05607449 0.94392544]\n",
      "[0.02512425 0.97487575]\n",
      "[0.9730843  0.02691572]\n",
      "[0.03373832 0.9662616 ]\n",
      "[0.03373832 0.9662616 ]\n",
      "[0.84584206 0.15415795]\n",
      "[0.30881542 0.6911845 ]\n",
      "[0.97434694 0.02565305]\n",
      "[0.02912395 0.97087604]\n",
      "[0.9668236  0.03317642]\n",
      "[0.97176117 0.02823875]\n",
      "[0.97381306 0.02618697]\n",
      "[0.9607592  0.03924081]\n",
      "[0.02512425 0.97487575]\n",
      "[0.02912395 0.97087604]\n",
      "[0.02166159 0.9783384 ]\n",
      "[0.97176117 0.02823875]\n",
      "[0.5075853  0.49241477]\n",
      "[0.84584206 0.15415795]\n",
      "[0.02705233 0.97294766]\n",
      "[0.97426164 0.02573837]\n",
      "[0.03905441 0.96094555]\n",
      "[0.27430987 0.72569007]\n",
      "[0.6299371 0.3700629]\n",
      "[0.6679988 0.3320012]\n",
      "[0.18626446 0.8137356 ]\n",
      "[0.01866703 0.98133296]\n",
      "[0.0798953 0.9201047]\n",
      "[0.03134909 0.96865094]\n",
      "[0.04855872 0.9514412 ]\n",
      "[0.9727126  0.02728745]\n",
      "[0.18626446 0.8137356 ]\n",
      "[0.02705233 0.97294766]\n",
      "[0.84584206 0.15415795]\n",
      "[0.0521889 0.9478111]\n",
      "[0.9512524  0.04874762]\n",
      "[0.9725248  0.02747519]\n",
      "[0.96414065 0.03585937]\n",
      "[0.0233303  0.97666967]\n",
      "[0.03373832 0.9662616 ]\n",
      "[0.0521889 0.9478111]\n",
      "[0.0521889 0.9478111]\n",
      "[0.03630286 0.9636972 ]\n",
      "[0.03134909 0.96865094]\n",
      "[0.07448935 0.92551064]\n",
      "[0.07448935 0.92551064]\n"
     ]
    }
   ],
   "source": [
    "for  i in  predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "rounded_predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "for i in rounded_predictions:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
